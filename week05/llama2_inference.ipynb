{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "886ed769-07f9-474f-9f86-c9f1f17345e4",
   "metadata": {},
   "source": [
    "## 使用微调后的 LLaMA2-7B 推理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24f9cf61-2994-48c7-9a42-150920397a93",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1c571fa5-aa51-495c-8b3d-d5605a02e491",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/.pyenv/versions/3.11.1/lib/python3.11/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.04s/it]\n",
      "/root/.pyenv/versions/3.11.1/lib/python3.11/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from peft import AutoPeftModelForCausalLM\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "\n",
    "model_dir = \"models/llama-7-int4-dolly-20250818_005604\"\n",
    " \n",
    "# 加载基础LLM模型与分词器\n",
    "model = AutoPeftModelForCausalLM.from_pretrained(\n",
    "    model_dir,\n",
    "    low_cpu_mem_usage=True,\n",
    "    torch_dtype=torch.float16,\n",
    "    load_in_4bit=True,\n",
    ") \n",
    "tokenizer = AutoTokenizer.from_pretrained(model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7700c042-20cf-4ff1-8c40-a91cabeaaaa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "/root/.pyenv/versions/3.11.1/lib/python3.11/site-packages/bitsandbytes/nn/modules.py:226: UserWarning: Input type into Linear4bit is torch.float16, but bnb_4bit_compute_type=torch.float32 (default). This will lead to slow inference or training speed.\n",
      "  warnings.warn(f'Input type into Linear4bit is torch.float16, but bnb_4bit_compute_type=torch.float32 (default). This will lead to slow inference or training speed.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt:\n",
      "The Primacy of the Spains (Portuguese: Primaz das Espanhas; Spanish: Primado de las Españas, Catalan: Primat de les Espanyes) is the primacy of the Iberian Peninsula, historically known as Hispania or in the plural as the Spains. \n",
      "\n",
      "The Archbishop of Braga, in Portugal, has claimed this primacy over the whole Iberian Peninsula since the middle ages, however today his primacy is only recognized in Portugal. The Archbishop of Toledo in Spain has claimed the Primacy of Spain, as the primate above all other episcopal sees in Spain. \n",
      "\n",
      "In addition, the Archbishop of Tarragona in Catalonia also make use of the title. The Archbishops in Braga, Toledo and Tarragona, if raised to the rank of cardinal, are known as Cardinal-Primates.\n",
      "\n",
      "Generated instruction:\n",
      "Who are the Primacy of the Spains?\n",
      "\n",
      "Ground truth:\n",
      "Please explain the Primacy of the Spains\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset \n",
    "from random import randrange\n",
    " \n",
    " \n",
    "# 从hub加载数据集并得到一个样本\n",
    "dataset = load_dataset(\"databricks/databricks-dolly-15k\", split=\"train\")\n",
    "sample = dataset[randrange(len(dataset))]\n",
    " \n",
    "prompt = f\"\"\"### Instruction:\n",
    "Use the Input below to create an instruction, which could have been used to generate the input using an LLM. \n",
    " \n",
    "### Input:\n",
    "{sample['response']}\n",
    " \n",
    "### Response:\n",
    "\"\"\"\n",
    " \n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\", truncation=True).input_ids.cuda()\n",
    "\n",
    "outputs = model.generate(input_ids=input_ids, max_new_tokens=100, do_sample=True, top_p=0.9,temperature=0.9)\n",
    "\n",
    "print(f\"Prompt:\\n{sample['response']}\\n\")\n",
    "print(f\"Generated instruction:\\n{tokenizer.batch_decode(outputs.detach().cpu().numpy(), skip_special_tokens=True)[0][len(prompt):]}\")\n",
    "print(f\"Ground truth:\\n{sample['instruction']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f6ea5b0-cf32-4cc0-bd5e-fc17268992c0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
